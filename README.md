# Text-Summarization
# Introduction
The main idea was to implement an automatic text generation system that is capable of extracting key information from selected academic papers. Researchers spend hours of time over papers during the course of their research. This solution would optimise this by providing concise and informative summariers of the papers.

The challenge with summarizing text comes from not having enough labeled training data. Researchers have tackled this issue in a paper called "**A Supervised Approach to Extractive Summarisation of Scientific Papers**" [1]. They suggest a way to mine scientific papers and create initial summaries for training data. They also introduce methods, like scoring sentences based on word overlap, and more advanced ones like Recurrent Neural Networks (RNNs) and Long Short Term Memory (LSTM) units. This paper was the basis for the project.

Recurrent Neural Networks (RNNs) are commonly used in text generation because they handle the sequence of words well. However, they struggle with remembering long-term connections between words. Long Short Term Memory (LSTM) units solve this problem by paying attention to previous words in a sentence. This helps create more natural-sounding summaries that consider the context of words. 

Today, the best methods in Natural Language Processing use Transformers like BERT (by Google) [7] and GPT-2 (by OpenAI). These models improve on LSTM's idea of attention by looking at words before and after a word. Our project doesn't reach the same level as these advanced models, but it shows the difference between extractive and abstractive summarization methods. Using Transformers could be a future step for our work.

So, the project gives a comparison of extractive and abstractive summarization methods. It sets the stage for possible advancements in Natural Language Processing, aiming to contribute to progress in the field.

# Extractive Summarization
The extractive approach  aimed to summarize academic papers by exploring various methods. Dataset sourced from Ed Collins, et al [1], extracted from ScienceDirect was used. This dataset consisted of summaries, referred to as "Highlights", authored by the original researchers. The initial strategy involved scoring each sentence based on the common words shared between the paper's main body and the author-provided summaries. The top "n" sentences with the highest scores were selected as summary sentences.

To accomplish this, a Support Vector Machine Regressor was used to predict sentence scores using document vectors generated by Gensim's doc2vec function. Rouge scores were used to quantify the results.

# Abstractive Summarization
After encountering limitations with extractive summarization approach, the focus was shifted to abstractive summarization. This transition allowed to address issues related to word placement and inter-word dependencies, which were not fully accounted for previously. To accomplish this, a Neural Network using TensorFlow and Keras was implemented. Also, the complexity of Neural Network was increased by adding three layers. This adjustment was essential to capture a more comprehensive range of latent information within the words and their context.

Therefore, he shift to abstractive summarization proved beneficial as it allowed the model to generate summaries that captured contextual information and word dependencies, resulting in more coherent and informative summaries compared to the extractive approach.

# Conclusion
In the exploration of text summarization models, two distinct approaches were developed: one relying on heuristics and the other utilizing deep recurrent neural networks. These approaches yielded markedly different results, highlighting the impact of methodology on the quality of summaries. Given more extensive training data, computational resources, and training time, improved performance can be anticipated. The comparison of the two approaches emphasized their respective strengths and weaknesses in generating summaries. Moving forward, the project's potential lies in further iterations of model training with increased data and exploration of advanced deep learning techniques such as transformers, particularly BERT.



